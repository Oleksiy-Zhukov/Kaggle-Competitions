{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/zhukovoleksiy/5-solution-ps3e13-ensemble?scriptVersionId=144000048\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"*September 23, 2023 - linked to GitHub*","metadata":{}},{"cell_type":"markdown","source":"<a id='top'></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\"></div>\n\n<h2 style=\"text-align: left; background-color:#1192AA; font-family: Georgia; color: #EEE8A9; padding: 13px; line-height:0.9;border-radius:2px; margin-bottom: 0em; text-align: center; font-size: 25px; border-radius: 100px 100px;\">TABLE OF CONTENTS</h2>\n\n* &nbsp; **[Import](#IMPORT-AND-LOAD-DATA)**\n\n\n* &nbsp; **[Load Data](#Load-Data)**\n\n\n* &nbsp; **[Basic EDA](#EDA)**\n\n\n* &nbsp; **[Feature Engineering](#Feature-Engineering)**\n\n\n* &nbsp; **[Model Building](#Model-Building)**\n\n\n* &nbsp; **[Make Submission](#Make-Submission)**\n\n\n* &nbsp; **[Acknowledgements](#Acknowledgements)**\n","metadata":{}},{"cell_type":"markdown","source":"# <h2 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Import</h2>","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport os\nfrom copy import deepcopy\nfrom functools import partial\nfrom itertools import combinations\nimport random\nimport gc\n\n# Import sklearn classes for model selection, cross validation, and performance evaluation\n#from sklearn.base import BaseEstimator, TransformerMixin\nfrom category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import roc_auc_score, accuracy_score, log_loss\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.decomposition import PCA, NMF\nfrom umap import UMAP\nfrom sklearn.manifold import TSNE\n\n# Import libraries for Hypertuning\nimport optuna\nfrom joblib import Parallel, delayed\n\n# More models\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom sklearn.svm import NuSVC, SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\nfrom catboost import Pool\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-29T00:51:54.934732Z","iopub.execute_input":"2023-04-29T00:51:54.935306Z","iopub.status.idle":"2023-04-29T00:51:54.952722Z","shell.execute_reply.started":"2023-04-29T00:51:54.935248Z","shell.execute_reply":"2023-04-29T00:51:54.951385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h2 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Load Data</h2>","metadata":{}},{"cell_type":"code","source":"filepath = '/kaggle/input/playground-series-s3e13'\norigin_filepath = '/kaggle/input/vector-borne-disease-prediction/trainn.csv'\n\ndf_train = pd.read_csv(os.path.join(filepath, 'train.csv'), index_col=[0])\ndf_test = pd.read_csv(os.path.join(filepath, 'test.csv'), index_col=[0])\n\noriginal = pd.read_csv(origin_filepath)\noriginal.prognosis = original.prognosis.str.replace(' ', '_')\n\ndf_train['is_generated'] = 1\ndf_test['is_generated'] = 1\noriginal['is_generated'] = 0\n\noriginal.prognosis = original.prognosis.str.replace(' ', '_')\ndf_concat = pd.concat([df_train, original], axis=0).reset_index(drop=True)\n\n# df_concat = df_train.copy()\n\ntarget_col = 'prognosis'","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:51:54.955488Z","iopub.execute_input":"2023-04-29T00:51:54.956433Z","iopub.status.idle":"2023-04-29T00:51:55.010288Z","shell.execute_reply.started":"2023-04-29T00:51:54.956381Z","shell.execute_reply":"2023-04-29T00:51:55.009339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h2 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">EDA</h2>","metadata":{}},{"cell_type":"code","source":"# Create figure\nfig = px.histogram(df_train['prognosis'],\n                   template='plotly_dark',\n                   color_discrete_sequence = ['#1192AA'])\n\n\n\n# Set Title and x/y axis labels\nfig.update_layout(\n    xaxis_title=\"Disease\",\n    yaxis_title=\"Frequency\",\n    showlegend = False,\n    font = dict(\n            size = 14\n            ),    \n    title={\n        'text': \"Train Prognosis Distribution\",\n        'y':0.95,\n        'x':0.5\n        }\n    )\n\n# Display\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:51:55.012045Z","iopub.execute_input":"2023-04-29T00:51:55.012839Z","iopub.status.idle":"2023-04-29T00:51:55.087374Z","shell.execute_reply.started":"2023-04-29T00:51:55.012787Z","shell.execute_reply":"2023-04-29T00:51:55.086107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create figure\nfig = px.histogram(original['prognosis'],\n                   template='plotly_dark',\n                   color_discrete_sequence = ['#1192AA'])\n\n\n\n# Set Title and x/y axis labels\nfig.update_layout(\n    xaxis_title=\"Disease\",\n    yaxis_title=\"Frequency\",\n    showlegend = False,\n    font = dict(\n            size = 14\n            ),    \n    title={\n        'text': \"Oririn Prognosis Distribution\",\n        'y':0.95,\n        'x':0.5\n        }\n    )\n\n# Display\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:51:55.089293Z","iopub.execute_input":"2023-04-29T00:51:55.090066Z","iopub.status.idle":"2023-04-29T00:51:55.163474Z","shell.execute_reply.started":"2023-04-29T00:51:55.090015Z","shell.execute_reply":"2023-04-29T00:51:55.162096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create figure\nfig = px.imshow(df_train.corr(), template='plotly_dark')\n\n# Set Title and x/y axis labels\nfig.update_layout(\n    showlegend = False,\n    font = dict(\n            size = 14\n            ),    \n    title={\n        'text': \"Train Dataset Correlation\",\n        'y':0.95,\n        'x':0.49\n        }\n    )\n\n# Display\nfig.show() ","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:51:55.166422Z","iopub.execute_input":"2023-04-29T00:51:55.166849Z","iopub.status.idle":"2023-04-29T00:51:55.232284Z","shell.execute_reply.started":"2023-04-29T00:51:55.166809Z","shell.execute_reply":"2023-04-29T00:51:55.230989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create figure\nfig = px.imshow(original.corr(), template='plotly_dark')\n\n# Set Title and x/y axis labels\nfig.update_layout(\n    showlegend = False,\n    font = dict(\n            size = 14\n            ),    \n    title={\n        'text': \"Original Dataset Correlation\",\n        'y':0.95,\n        'x':0.49\n        }\n    )\n\n# Display\nfig.show() ","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:51:55.234149Z","iopub.execute_input":"2023-04-29T00:51:55.234569Z","iopub.status.idle":"2023-04-29T00:51:55.295253Z","shell.execute_reply.started":"2023-04-29T00:51:55.234533Z","shell.execute_reply":"2023-04-29T00:51:55.293877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_countplots(df, num_cols, palette):\n    num_rows = (len(df_test.columns)) // num_cols\n    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(18, 4*num_rows))\n    sns.set(font_scale=1.2, style='white')\n\n    for i, col_name in enumerate(df_test.columns):\n        #if (col_name != 'is_generated') or (col_name != target_col):\n        ax = axes[(i-1) // num_cols, (i-1) % num_cols]\n        sns.countplot(data=df, x=col_name, ax=ax, palette=palette)\n        ax.set_title(f'{col_name.title()}', fontsize=18)\n        ax.set_xlabel(col_name.title(), fontsize=14)\n        ax.tick_params(axis='both', which='major', labelsize=12)\n        sns.despine()\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:51:55.296852Z","iopub.execute_input":"2023-04-29T00:51:55.297956Z","iopub.status.idle":"2023-04-29T00:51:55.308191Z","shell.execute_reply.started":"2023-04-29T00:51:55.297904Z","shell.execute_reply":"2023-04-29T00:51:55.306933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_palette = ['#1192AA','#3E2756']\nplot_countplots(pd.concat([df_train, original], axis=0).reset_index(drop=True), 4, my_palette)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:51:55.309451Z","iopub.execute_input":"2023-04-29T00:51:55.309794Z","iopub.status.idle":"2023-04-29T00:52:03.907325Z","shell.execute_reply.started":"2023-04-29T00:51:55.309761Z","shell.execute_reply":"2023-04-29T00:52:03.906024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h2 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Feature Engineering</h2>","metadata":{}},{"cell_type":"code","source":"# # https://www.kaggle.com/code/sergiosaharovskiy/ps-s3e13-2023-eda-and-submission\n# pain = df_concat.columns[df_concat.columns.str.contains('pain')]\n# inflammation = df_concat.columns[df_concat.columns.str.contains('inflammation')]\n# bleeding = df_concat.columns[df_concat.columns.str.contains('bleed')]\n# skin = df_concat.columns[df_concat.columns.str.contains('bleed')].tolist() + ['itchiness']\n\nsimilar_columns = [\n    'loss_of_appetite',\n    'urination_loss',\n    'slow_heart_rate',\n    'abdominal_pain', \n    'light_sensitivity',\n    'yellow_skin',\n    'yellow_eyes'\n]\n\nchikungunya_columns = [\n    'convulsion',\n    'finger_inflammation',\n    'speech_problem',\n    'toenail_loss',\n    'ulcers', \n    'itchiness',\n    'lips_irritation',\n    'breathing_restriction',\n    'toe_inflammation',\n    'paralysis',\n    'stomach_pain',\n    'confusion',\n    'irritability',\n    'bullseye_rash'\n]\n\nlyme_columns = [\n    'jaundice',\n    'weight_loss',\n    'weakness',\n    'back_pain',\n    'sudden_fever', \n    'myalgia',\n    'chills',\n    'orbital_pain',\n    'digestion_trouble'\n]\n\nred_cols = [\n    'diarrhea',\n    'hypotension',\n    'pleural_effusion',\n    'ascites', \n    'gastro_bleeding',\n    'swelling',\n    'nausea', \n    'chills', \n    'myalgia', \n    'digestion_trouble',\n    'fatigue', \n    'skin_lesions', \n    'stomach_pain',\n    'orbital_pain',\n    'neck_pain', \n    'weakness', \n    'back_pain', \n    'weight_loss', \n    'gum_bleed', \n    'jaundice', \n    'coma',\n    'diziness',\n    'inflammation',\n    'red_eyes',\n    'loss_of_appetite',\n    'urination_loss',\n    'slow_heart_rate',\n    'abdominal_pain', \n    'light_sensitivity',\n    'yellow_skin',\n    'yellow_eyes', \n    'facial_distortion', \n    'microcephaly'\n]\n\norange_cols = [\n    'rigor',\n    'bitter_tongue',\n    'convulsion',\n    'anemia',\n    'cocacola_urine',\n    'hypoglycemia',\n    'prostraction',\n    'hyperpyrexia',\n    'stiff_neck',\n    'irritability',\n    'confusion',\n    'tremor',\n    'paralysis',\n    'lymph_swells',\n    'breathing_restriction',\n    'toe_inflammation',\n    'finger_inflammation',\n    'lips_irritation',\n    'itchiness',\n    'ulcers',\n    'toenail_loss',\n    'speech_problem',\n    'bullseye_rash'\n]\n\ngreen_cols = [\n    'sudden_fever',\n    'headache',\n    'mouth_bleed',\n    'nose_bleed',\n    'muscle_pain',\n    'joint_pain',\n    'vomiting',\n    'rash']\n\n# Implementing features\nfor df in [df_concat, df_test]:\n    df['similar_cluster'] = df[similar_columns].sum(axis=1)\n    df['chikungunya_columns'] = df[chikungunya_columns].sum(axis=1)\n    df['lyme_columns'] = df[lyme_columns].sum(axis=1)\n#     df['pain'] = df[pain].sum(axis=1)\n#     df['skin'] = df[skin].sum(axis=1)\n#     df['inflammation'] = df[inflammation].sum(axis=1)\n#     df['bleeding'] = df[bleeding].sum(axis=1)\n    df['red_cols'] = df[red_cols].sum(axis=1)\n    df['orange_cols'] = df[orange_cols].sum(axis=1)\n    df['green_cols'] = df[green_cols].sum(axis=1)\n\nfor df in [df_concat, df_test]: \n    tungiasis_columns = ['ulcers', 'toenail_loss', 'itchiness']\n    df['tungiasis_cluster'] = df[tungiasis_columns].sum(axis=1)\n    \n    columns = [col for col in df if col != 'prognosis']\n    df[columns] = df[columns].astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:52:03.909279Z","iopub.execute_input":"2023-04-29T00:52:03.909638Z","iopub.status.idle":"2023-04-29T00:52:03.99734Z","shell.execute_reply.started":"2023-04-29T00:52:03.909602Z","shell.execute_reply":"2023-04-29T00:52:03.996245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decomp:","metadata":{}},{"cell_type":"code","source":"class Decomp:\n    def __init__(self, n_components, method=\"pca\", scaler_method='standard'):\n        self.n_components = n_components\n        self.method = method\n        self.scaler_method = scaler_method\n        \n    def dimension_reduction(self, df):\n            \n        X_reduced = self.dimension_method(df)\n        df_comp = pd.DataFrame(X_reduced, columns=[f'{self.method.upper()}_{_}' for _ in range(self.n_components)], index=df.index)\n        \n        return df_comp\n    \n    def dimension_method(self, df):\n        \n        X = self.scaler(df)\n        if self.method == \"pca\":\n            comp = PCA(n_components=self.n_components, random_state=0)\n            X_reduced = comp.fit_transform(X)\n        elif self.method == \"nmf\":\n            comp = NMF(n_components=self.n_components, random_state=0)\n            X_reduced = comp.fit_transform(X)\n        elif self.method == \"umap\":\n            comp = UMAP(n_components=self.n_components, random_state=0)\n            X_reduced = comp.fit_transform(X)\n        elif self.method == \"tsne\":\n            comp = TSNE(n_components=self.n_components, random_state=0) # Recommend n_components=2\n            X_reduced = comp.fit_transform(X)\n        else:\n            raise ValueError(f\"Invalid method name: {method}\")\n        \n        self.comp = comp\n        return X_reduced\n    \n    def scaler(self, df):\n        \n        _df = df.copy()\n            \n        if self.scaler_method == \"standard\":\n            return StandardScaler().fit_transform(_df)\n        elif self.scaler_method == \"minmax\":\n            return MinMaxScaler().fit_transform(_df)\n        elif self.scaler_method == None:\n            return _df.values\n        else:\n            raise ValueError(f\"Invalid scaler_method name\")\n        \n    def get_columns(self):\n        return [f'{self.method.upper()}_{_}' for _ in range(self.n_components)]\n    \n    def transform(self, df):\n        X = self.scaler(df)\n        X_reduced = self.comp.transform(X)\n        df_comp = pd.DataFrame(X_reduced, columns=[f'{self.method.upper()}_{_}' for _ in range(self.n_components)], index=df.index)\n        \n        return df_comp\n    \n    @property\n    def get_explained_variance_ratio(self):\n        \n        return np.sum(self.comp.explained_variance_ratio_)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:52:03.999269Z","iopub.execute_input":"2023-04-29T00:52:03.999781Z","iopub.status.idle":"2023-04-29T00:52:04.016956Z","shell.execute_reply.started":"2023-04-29T00:52:03.999731Z","shell.execute_reply":"2023-04-29T00:52:04.015543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dictionary mapping each disease name to a corresponding integer value\ntarget_map = {\n    'Lyme_disease': 0,\n    'Tungiasis': 1,\n    'Zika': 2,\n    'Rift_Valley_fever': 3,\n    'West_Nile_fever': 4,\n    'Malaria': 5,\n    'Chikungunya': 6,\n    'Plague': 7,\n    'Dengue': 8,\n    'Yellow_Fever': 9,\n    'Japanese_encephalitis': 10\n}\nswapped_map = {v: k for k, v in target_map.items()}\ndf_concat[target_col] = df_concat[target_col].replace(target_map).astype(int)\n\n# Concatenate train and original dataframes, and prepare train and test sets\ndrop_columns = [] # 'is_generated'\nX_train = df_concat.drop([target_col]+drop_columns, axis=1).reset_index(drop=True).astype(int)\ny_train = df_concat[target_col].reset_index(drop=True)\nX_test = df_test.drop(drop_columns, axis=1).reset_index(drop=True).astype(int)\n\n# X_train_ori, X_test_ori = X_train.copy(), X_test.copy()\n\n# # Add dimension_reduction Featrues\n# umap_n_components = 5\n# decomp = Decomp(n_components=umap_n_components, method='umap', scaler_method=None)\n# umap_train = decomp.dimension_reduction(X_train).reset_index(drop=True)\n# umap_test = decomp.transform(X_test).reset_index(drop=True)\n# print(f'  --> UMAP(n_components={umap_n_components})')\n\n# # Concat Data\n# X_train = pd.concat([X_train_ori, umap_train], axis=1).reset_index(drop=True)\n# X_test = pd.concat([X_test_ori, umap_test], axis=1).reset_index(drop=True)\n# # X_train, X_test = X_train_ori.copy(), X_test_ori.copy()\n\nprint(\"\")\nprint(f\"X_train shape :{X_train.shape}\")\nprint(f\"y_train shape :{y_train.shape}\")\nprint(f\"X_test shape :{X_test.shape}\")\n\n# Delete the train and test dataframes to free up memory\ndel df_train, df_test, original\n\nX_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:52:04.018553Z","iopub.execute_input":"2023-04-29T00:52:04.01906Z","iopub.status.idle":"2023-04-29T00:52:08.850326Z","shell.execute_reply.started":"2023-04-29T00:52:04.019021Z","shell.execute_reply":"2023-04-29T00:52:08.849064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h2 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Model Building</h2>","metadata":{}},{"cell_type":"markdown","source":"<h2 style = \"font-family: Georgia;font-weight: bold; font-size: 19px; color: #1192AA; text-align:left\">Optimizer (MAP@3):</h2>","metadata":{}},{"cell_type":"code","source":"#https://www.kaggle.com/code/nandeshwar/mean-average-precision-map-k-metric-explained-code/notebook\ndef apk(actual, predicted, k=10):\n    if not actual:\n        return 0.0\n\n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i,p in enumerate(predicted):\n        # first condition checks whether it is valid prediction\n        # second condition checks if prediction is not repeated\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    return score / min(len(actual), k)\n\ndef mapk(actual, predicted, k=10):\n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"font-family: Georgia;font-weight: bold; font-size: 19px; color: #1192AA; text-align:left\">Splitter:</h2>","metadata":{}},{"cell_type":"code","source":"class Splitter:\n    def __init__(self, kfold=True, n_splits=10):\n        self.n_splits = n_splits\n        self.kfold = kfold\n\n    def split_data(self, X, y, random_state_list):\n        if self.kfold:\n            for random_state in random_state_list:\n                kf = StratifiedKFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n                for train_index, val_index in kf.split(X, y):\n                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n                    yield X_train, X_val, y_train, y_val\n        else:\n            raise ValueError(f\"Invalid kfold: Must be True\")","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:52:08.864127Z","iopub.execute_input":"2023-04-29T00:52:08.864623Z","iopub.status.idle":"2023-04-29T00:52:08.88384Z","shell.execute_reply.started":"2023-04-29T00:52:08.864571Z","shell.execute_reply":"2023-04-29T00:52:08.882777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"font-family: Georgia;font-weight: bold; font-size: 19px; color: #1192AA; text-align:left\">XGBoost, LGBM and CatBoost Hypertune:</h2>","metadata":{}},{"cell_type":"markdown","source":"This is code I used to hypertune my models, I also tried to hypertune them on different datasets with different feature engineering approaches.\n\n```python\n# Load data and define target and features\ndata = pd.DataFrame(y_train).join(X_train)\ntarget = y_train\nfeatures = X_train\n\n# Define params\nn_splits = 10\nn_trials = 100\nearly_stopping_rounds = 333\n\n# Define objective function for Optuna\ndef objective(trial):\n    # Define hyperparameters to optimize\n    xgb_params = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 1000, step=50),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 1.0),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1.0),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.001, 10.0),\n        'n_jobs': -1,\n        'eval_metric': 'mlogloss',\n        'objective': 'multi:softprob',\n        'tree_method': 'hist',\n        'verbosity': 0,\n        'random_state': 42\n     }\n    \n    lgb_params = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 1000, step=50),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 1.0),\n        'subsample': trial.suggest_uniform('subsample', 0.1, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1.0),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.001, 10.0),\n        'one_hot_max_size': trial.suggest_int('one_hot_max_size', 10, 100),\n        'objective': 'multiclass',\n        'metric': 'multi_logloss',\n        'boosting_type': 'gbdt',\n        'device': \"cpu\",\n        'random_state': 42\n    }\n    \n    cat_params = {\n        'iterations': self.n_estimators,\n        'depth': trial.suggest_int('depth', 4, 10),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.5),\n        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 0.1, 10.0),\n        'random_strength': trial.suggest_loguniform('random_strength', 0.1, 1.0),\n        'max_bin': trial.suggest_int('max_bin', 100, 500),\n        'od_wait': trial.suggest_int('od_wait', 20, 100),\n        'one_hot_max_size': 70,\n        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli']),\n        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n        'eval_metric': 'MultiClass',\n        'loss_function': 'MultiClass',\n        'random_state': 42\n    }\n        \n    # Initialize KFold cross-validator\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    \n    # Initialize list to store MAP@3 scores for each fold\n    map3_scores = []\n    \n    # Loop over folds in parallel\n    results = Parallel(n_jobs=-1)(delayed(get_fold_score)(params, features, target, train_idx, valid_idx, early_stopping_rounds) for train_idx, valid_idx in kf.split(data))\n    map3_scores = [r[0] for r in results]\n    \n    # Return negative mean of MAP@3 scores (Optuna maximizes the objective)\n    return np.mean(map3_scores)\n\n# Define function to get MAP@3 score for a fold\ndef get_fold_score(params, features, target, train_idx, valid_idx, early_stopping_rounds):\n    # Split data into training and validation sets\n    X_train, X_valid = features.iloc[train_idx], features.iloc[valid_idx]\n    y_train, y_valid = target.iloc[train_idx], target.iloc[valid_idx]\n\n    # Initialize XGBoost model\n    model = xgb.XGBClassifier(**params)\n    \n#    model = lgb.LGBMClassifier(**params)\n    \n#    model = CatBoostClassifier(**params)\n    \n   \n    # Fit model on training data\n    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=early_stopping_rounds, verbose=False)\n\n    # Predict probabilities for validation data\n    y_pred = model.predict_proba(X_valid)\n    top_preds = np.argsort(-y_pred, axis=1)[:, :3]\n\n    # Calculate MAP@3 score for validation data\n    map3 = mapk(y_valid.values.reshape(-1, 1), top_preds, 3)\n\n    return (map3,)\n\n# Run Optuna to find the best hyperparameters\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=n_trials)\n\n# Print the best hyperparameters and their score\nprint('Best hyperparameters: {}'.format(study.best_params))\nprint('Best score: {}'.format(study.best_value))\n```","metadata":{}},{"cell_type":"markdown","source":"<h2 style = \"font-family: Georgia;font-weight: bold; font-size: 19px; color: #1192AA; text-align:left\">Define Models:</h2>","metadata":{}},{"cell_type":"code","source":"class Classifier:\n    def __init__(self, n_estimators=250, device=\"cpu\", random_state=0):\n        self.n_estimators = n_estimators\n        self.device = device\n        self.random_state = random_state\n        self.models = self._define_model()\n        self.len_models = len(self.models)\n        \n    def _define_model(self):\n        \n        xgb_params = {\n            'n_estimators': 950,\n            'learning_rate': 0.15639672008038652,\n            'max_depth': 9,\n            'subsample': 0.7154363211099006,\n            'colsample_bytree': 0.1834688802646254,\n            'reg_alpha': 0.00662736159424831,\n            'reg_lambda': 0.392111943900896,\n            'n_jobs': -1,\n            'eval_metric': 'mlogloss',\n            'objective': 'multi:softprob',\n            'tree_method': 'hist',\n            'verbosity': 0,\n            'random_state': self.random_state\n        }\n        \n        if self.device == 'gpu':\n            xgb_params['tree_method'] = 'gpu_hist'\n            xgb_params['predictor'] = 'gpu_predictor'\n        \n        \n        lgb_params = {\n            'n_estimators': 650,\n            'max_depth': 6,\n            'learning_rate': 0.09800555723996654,\n            'subsample': 0.5113976179887376,\n            'colsample_bytree': 0.15594697300978008,\n            'reg_alpha': 0.24312642991831,\n            'reg_lambda': 0.06500132210882924,\n            'one_hot_max_size': 95,\n            'device': self.device,\n            'random_state': self.random_state}\n                \n        \n        cb_params = {\n            'n_estimators': 1000,\n            'depth': 9, \n            'learning_rate': 0.45645253367049604,\n            'l2_leaf_reg': 8.407202048380578,\n            'random_strength': 0.1793388390086202,\n            'max_bin': 225, \n            'od_wait': 58, \n            'grow_policy': 'Lossguide',\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'Iter',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state\n        }\n                \n        models = {\n            'svc': SVC(gamma=\"auto\", probability=True, random_state=self.random_state),\n#             'svc_li': SVC(kernel=\"linear\", gamma=\"auto\", probability=True, random_state=self.random_state),\n#             'svc_po': SVC(kernel=\"poly\", gamma=\"auto\", probability=True, random_state=self.random_state),\n#             'svc_si': SVC(kernel=\"sigmoid\", gamma=\"auto\", probability=True, random_state=self.random_state),\n            'lr': LogisticRegression(max_iter=150, random_state=self.random_state, n_jobs=-1),\n            'xgb': xgb.XGBClassifier(**xgb_params),\n            'lgb': lgb.LGBMClassifier(**lgb_params),\n            'cat': CatBoostClassifier(**cb_params),\n            'brf': BalancedRandomForestClassifier(n_estimators=400, n_jobs=-1, random_state=self.random_state),\n            'rf': RandomForestClassifier(n_estimators=200, random_state=self.random_state),\n#            'knn': KNeighborsClassifier(n_neighbors=5),\n#            'mlp': MLPClassifier(random_state=self.random_state, max_iter=1000),\n#            'xgb_0':xgb.XGBClassifier(random_state=self.random_state),\n#             'lgb_0':lgb.LGBMClassifier(random_state=self.random_state),\n#             'cat_0':CatBoostClassifier(random_state=self.random_state),\n        }\n        \n        return models","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:52:08.890398Z","iopub.execute_input":"2023-04-29T00:52:08.891374Z","iopub.status.idle":"2023-04-29T00:52:08.906714Z","shell.execute_reply.started":"2023-04-29T00:52:08.891328Z","shell.execute_reply":"2023-04-29T00:52:08.90564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"font-family: Georgia;font-weight: bold; font-size: 19px; color: #1192AA; text-align:left\">Ensemble Weights:</h2>","metadata":{}},{"cell_type":"code","source":"class OptunaWeights:\n    def __init__(self, random_state, n_trials=2000):\n        self.study = None\n        self.weights = None\n        self.random_state = random_state\n        self.n_trials = n_trials\n\n    def _objective(self, trial, y_true, y_preds):\n        # Define the weights for the predictions from each model\n        weights = [trial.suggest_float(f\"weight{n}\", 1e-12, 1) for n in range(len(y_preds))]\n\n        # Calculate the weighted prediction\n        weighted_pred = np.average(np.array(y_preds), axis=0, weights=weights)\n\n        # Calculate the MAP@3 score for the weighted prediction\n        top_preds = np.argsort(-weighted_pred, axis=1)[:, :3]\n        score = mapk(y_true.reshape(-1, 1), top_preds, 3)\n        \n        return score\n\n    def fit(self, y_true, y_preds):\n        optuna.logging.set_verbosity(optuna.logging.ERROR)\n        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n        self.study = optuna.create_study(sampler=sampler, study_name=\"OptunaWeights\", direction='maximize') # minimize\n        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n        self.study.optimize(objective_partial, n_trials=self.n_trials)\n        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n\n    def predict(self, y_preds):\n        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n        weighted_pred = np.average(np.array(y_preds), axis=0, weights=self.weights)\n        return weighted_pred\n\n    def fit_predict(self, y_true, y_preds):\n        self.fit(y_true, y_preds)\n        return self.predict(y_preds)\n    \n    def weights(self):\n        return self.weights","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:52:08.908111Z","iopub.execute_input":"2023-04-29T00:52:08.909092Z","iopub.status.idle":"2023-04-29T00:52:08.926629Z","shell.execute_reply.started":"2023-04-29T00:52:08.90905Z","shell.execute_reply":"2023-04-29T00:52:08.925398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"font-family: Georgia;font-weight: bold; font-size: 19px; color: #1192AA; text-align:left\">Train Model by K-fold:</h2>","metadata":{}},{"cell_type":"code","source":"kfold = True\nn_splits = 1 if not kfold else 12\nrandom_state = 42\nrandom_state_list = [42]\nn_estimators = 9999\nearly_stopping_rounds = 333\nverbose = False\ndevice = 'cpu'\natk = 3\n\nsplitter = Splitter(kfold=kfold, n_splits=n_splits)\n\n# Initialize an array for storing test predictions\ntest_predss = np.zeros((X_test.shape[0], len(y_train.unique())))\nensemble_score = []\nensemble_mapk_score = []\nweights = []\ntrained_models = {'xgb':[], 'lgb':[], 'cat':[], 'rf':[]}\nrank_df = pd.DataFrame(columns=swapped_map.keys(), index=X_test.index).fillna(0)\n\n    \nfor i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n    n = i % n_splits\n    m = i // n_splits\n            \n    # Get a set of Regressor models\n    classifier = Classifier(n_estimators, device, random_state)\n    models = classifier.models\n    \n    # Initialize lists to store oof and test predictions for each base model\n    oof_preds = []\n    test_preds = []\n    \n    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n    for name, model in models.items():\n        if name in ['xgb', 'lgb', 'cat', 'lgb2']:\n            model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n        else:\n            model.fit(X_train_, y_train_)\n            \n        if name in trained_models.keys():\n            trained_models[f'{name}'].append(deepcopy(model))\n        \n        test_pred = model.predict_proba(X_test)\n        y_val_pred = model.predict_proba(X_val)\n\n        top_preds = np.argsort(-y_val_pred, axis=1)[:, :atk]\n\n        mapk_score = mapk(y_val.values.reshape(-1, 1), top_preds, atk)\n        \n        score = log_loss(y_val, y_val_pred)\n        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] MAP@{atk}: {mapk_score:.5f}, Logloss: {score:.5f}')\n        \n        oof_preds.append(y_val_pred)\n        test_preds.append(test_pred)\n    \n    # Use Optuna to find the best ensemble weights\n    optweights = OptunaWeights(random_state=random_state)\n    y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n    \n    top_preds = np.argsort(-y_val_pred, axis=1)[:, :atk]\n    mapk_score = mapk(y_val.values.reshape(-1, 1), top_preds, atk)\n    \n    score = log_loss(y_val, y_val_pred)\n    print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] MAP@{atk}: {mapk_score:.5f}, Logloss: {score:.5f}')\n    \n    ensemble_score.append(score)\n    ensemble_mapk_score.append(mapk_score)\n    weights.append(optweights.weights)\n    \n    # Predict to X_test by the best ensemble weights\n    _test_preds = optweights.predict(test_preds)\n    test_predss += _test_preds / (n_splits * len(random_state_list))\n    \n    # Rank Prediction\n    for i in range(_test_preds.shape[0]):\n        arr = _test_preds[i]\n        sorted_indices = np.argsort(arr)\n        for k, p in zip(range(1, 10), [9, 8, 7, 6, 5, 4, 3, 2, 1]):\n            second_largest_index = sorted_indices[-k]\n            rank_df.loc[i, second_largest_index] += p\n    \n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T01:00:35.610036Z","iopub.execute_input":"2023-04-29T01:00:35.611336Z","iopub.status.idle":"2023-04-29T01:09:41.027049Z","shell.execute_reply.started":"2023-04-29T01:00:35.611288Z","shell.execute_reply":"2023-04-29T01:09:41.025723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"...","metadata":{}},{"cell_type":"code","source":"# Calculate the mean LogLoss score of the ensemble\nmean_score = np.mean(ensemble_score)\nmean_mapk_score = np.mean(ensemble_mapk_score)\nstd_score = np.std(ensemble_score)\nstd_mapk_score = np.std(ensemble_mapk_score)\nprint(f'Ensemble MAP@{atk} {mean_mapk_score:.5f} ± {std_mapk_score:.5f}')\nprint(f'Ensemble Logloss {mean_score:.5f} ± {std_score:.5f}')\n\n# Print the mean and standard deviation of the ensemble weights for each model\nprint('--- Model Weights ---')\nmean_weights = np.mean(weights, axis=0)\nstd_weights = np.std(weights, axis=0)\nfor name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n    print(f'{name}: {mean_weight:.5f} ± {std_weight:.5f}')","metadata":{"execution":{"iopub.status.busy":"2023-04-29T01:09:41.029375Z","iopub.execute_input":"2023-04-29T01:09:41.03026Z","iopub.status.idle":"2023-04-29T01:09:41.041957Z","shell.execute_reply.started":"2023-04-29T01:09:41.030209Z","shell.execute_reply":"2023-04-29T01:09:41.040784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"font-family: Georgia;font-weight: bold; font-size: 19px; color: #1192AA; text-align:left\">Visualize Feature importance (XGBoost, LightGBM, Catboost):</h2>","metadata":{}},{"cell_type":"code","source":"def visualize_importance(models, feature_cols, title, head=10):\n    importances = []\n    feature_importance = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df[\"importance\"] = model.feature_importances_\n        _df[\"feature\"] = pd.Series(feature_cols)\n        _df[\"fold\"] = i\n        _df = _df.sort_values('importance', ascending=False)\n        _df = _df.head(head)\n        feature_importance = pd.concat([feature_importance, _df], axis=0, ignore_index=True)\n        \n    feature_importance = feature_importance.sort_values('importance', ascending=False)\n    # display(feature_importance.groupby([\"feature\"]).mean().reset_index().drop('fold', axis=1))\n    plt.figure(figsize=(18, 8))\n    sns.barplot(x='importance', y='feature', data=feature_importance, color='#1192AA', errorbar='sd')\n    plt.xlabel('Importance', fontsize=14)\n    plt.ylabel('Feature', fontsize=14)\n    plt.title(f'{title} Feature Importance', fontsize=18)\n    plt.grid(True, axis='x')\n    plt.show()\n    \nfor name, models in trained_models.items():\n    visualize_importance(models, list(X_train.columns), name)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:59:48.562393Z","iopub.execute_input":"2023-04-29T00:59:48.562867Z","iopub.status.idle":"2023-04-29T00:59:51.391942Z","shell.execute_reply.started":"2023-04-29T00:59:48.562819Z","shell.execute_reply":"2023-04-29T00:59:51.390839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h2 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Make Submission</h2>","metadata":{}},{"cell_type":"code","source":"n_cols = 4\nn_rows = np.ceil(test_predss.shape[1] / n_cols).astype(int)\nfig, axs = plt.subplots(n_rows, n_cols, figsize=(15, 10))\naxs = axs.ravel()\n\nfor i in range(test_predss.shape[1]):\n    sns.histplot(data=test_predss[:, i], ax=axs[i], color='#1192AA')\n    axs[i].set_title(f\"{swapped_map[i]}\")\n    \nfig.suptitle(f'Ensemble softmax output', fontweight='bold')\nfig.tight_layout(pad=1.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:54:43.764612Z","iopub.execute_input":"2023-04-29T16:54:43.765141Z","iopub.status.idle":"2023-04-29T16:54:43.859964Z","shell.execute_reply.started":"2023-04-29T16:54:43.76509Z","shell.execute_reply":"2023-04-29T16:54:43.857946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(os.path.join(filepath, 'sample_submission.csv'))\nfor i in range(test_predss.shape[0]):\n    rank_list = list(rank_df.iloc[i, :].sort_values(0, ascending=False).iloc[:3].index)\n    sub.loc[i, 'prognosis'] = ' '.join([swapped_map[_] for _ in rank_list])\n    \n# Save submission file\nsub.to_csv('submission_rank.csv', index=False)\nsub","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:59:54.127177Z","iopub.execute_input":"2023-04-29T00:59:54.12762Z","iopub.status.idle":"2023-04-29T00:59:54.349367Z","shell.execute_reply.started":"2023-04-29T00:59:54.127572Z","shell.execute_reply":"2023-04-29T00:59:54.348164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h2 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Acknowledgements</h2>\n\n* Big credit to [@tetsu2131](https://www.kaggle.com/tetsutani) and his public notebook https://www.kaggle.com/code/tetsutani/ps3e13-eda-decomposition-ensemble-rankpredict as I used a lot of his code and used his notebook as a base for my work.\n* Thanks to [@belati](https://www.kaggle.com/belati) and his discussion topic https://www.kaggle.com/competitions/playground-series-s3e13/discussion/404958 for feature engineering ideas.\n* Thanks to [@sergiosaharovskiy](https://www.kaggle.com/sergiosaharovskiy) and his public notebook https://www.kaggle.com/code/sergiosaharovskiy/ps-s3e13-2023-eda-and-submission for great EDA and data insights.","metadata":{}}]}