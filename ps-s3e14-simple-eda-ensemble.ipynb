{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/zhukovoleksiy/ps-s3e14-simple-eda-ensemble?scriptVersionId=144818812\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"September 30 - linked to GitHub","metadata":{}},{"cell_type":"markdown","source":"# <h1 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Import</h1>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport os\nfrom copy import deepcopy\nfrom functools import partial\nfrom itertools import combinations\nimport random\nimport gc\n\n# Import sklearn classes for model selection, cross validation, and performance evaluation\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score, accuracy_score, log_loss\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nfrom category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA, NMF\nfrom umap import UMAP\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import mean_absolute_error\nfrom collections import defaultdict\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.ensemble import StackingRegressor\nfrom typing import List\n\n# Import libraries for Hypertuning\nimport optuna\n\n# Import libraries for gradient boosting\nimport xgboost as xgb\nimport lightgbm as lgb\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor, LassoCV\nfrom sklearn.linear_model import PassiveAggressiveRegressor, ARDRegression, RidgeCV, ElasticNetCV\nfrom sklearn.linear_model import TheilSenRegressor, RANSACRegressor, HuberRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cross_decomposition import PLSRegression\nfrom catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\nfrom catboost import Pool\n\n# Useful line of code to set the display option so we could see all the columns in pd dataframe\npd.set_option('display.max_columns', None)\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:10.605872Z","iopub.execute_input":"2023-05-05T01:45:10.606382Z","iopub.status.idle":"2023-05-05T01:45:34.146887Z","shell.execute_reply.started":"2023-05-05T01:45:10.606351Z","shell.execute_reply":"2023-05-05T01:45:34.145265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # <h1 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Check Dataset</h1>","metadata":{}},{"cell_type":"code","source":"PATH_ORIGIN = '/kaggle/input/wild-blueberry-yield-prediction-dataset/WildBlueberryPollinationSimulationData.csv'\nPATH_TRAIN = '/kaggle/input/playground-series-s3e14/train.csv'\nPATH_TEST = '/kaggle/input/playground-series-s3e14/test.csv'\nPATH_SUB = '/kaggle/input/playground-series-s3e14/sample_submission.csv'\n\ndf_train =  pd.read_csv(PATH_TRAIN).drop(columns='id')\ndf_test =   pd.read_csv(PATH_TEST).drop(columns='id')\noriginal = pd.read_csv(PATH_ORIGIN).drop(columns='Row#')\n\ntarget_col = 'yield'","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:34.149133Z","iopub.execute_input":"2023-05-05T01:45:34.150143Z","iopub.status.idle":"2023-05-05T01:45:34.275496Z","shell.execute_reply.started":"2023-05-05T01:45:34.15011Z","shell.execute_reply":"2023-05-05T01:45:34.274641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'[INFO] Shapes:'\n      f'\\n original: {original.shape}'\n      f'\\n train: {df_train.shape}'\n      f'\\n test: {df_test.shape}\\n')\n\nprint(f'[INFO] Any missing values:'\n      f'\\n original: {original.isna().any().any()}'\n      f'\\n train: {df_train.isna().any().any()}'\n      f'\\n test: {df_test.isna().any().any()}')","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:34.283194Z","iopub.execute_input":"2023-05-05T01:45:34.283847Z","iopub.status.idle":"2023-05-05T01:45:34.294725Z","shell.execute_reply.started":"2023-05-05T01:45:34.283817Z","shell.execute_reply":"2023-05-05T01:45:34.293298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_train = pd.concat([df_train, original], axis=0).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:34.296494Z","iopub.execute_input":"2023-05-05T01:45:34.297063Z","iopub.status.idle":"2023-05-05T01:45:34.304229Z","shell.execute_reply.started":"2023-05-05T01:45:34.297033Z","shell.execute_reply":"2023-05-05T01:45:34.303127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:34.305916Z","iopub.execute_input":"2023-05-05T01:45:34.306151Z","iopub.status.idle":"2023-05-05T01:45:34.33288Z","shell.execute_reply.started":"2023-05-05T01:45:34.30613Z","shell.execute_reply":"2023-05-05T01:45:34.331921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">EDA</h1>","metadata":{}},{"cell_type":"code","source":"# Create figure\nfig = px.histogram(x = df_train[target_col],\n                   template='simple_white',\n                   color_discrete_sequence = ['#1192AA'])\n\n\n\n# Set Title and x/y axis labels\nfig.update_layout(\n    xaxis_title=\"Yield Value\",\n    yaxis_title=\"Frequency\",\n    showlegend = False,\n    font = dict(\n            size = 14\n            ),    \n    title={\n        'text': \"Yield Distribution in `df_train`\",\n        'y':0.95,\n        'x':0.5\n        }\n    )\n\n# Display\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:34.334161Z","iopub.execute_input":"2023-05-05T01:45:34.334517Z","iopub.status.idle":"2023-05-05T01:45:35.723263Z","shell.execute_reply.started":"2023-05-05T01:45:34.334488Z","shell.execute_reply":"2023-05-05T01:45:35.721925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create figure\nfig = px.histogram(x = original[target_col],\n                   template='simple_white',\n                   color_discrete_sequence = ['#1192AA'])\n\n\n\n# Set Title and x/y axis labels\nfig.update_layout(\n    xaxis_title=\"Yield Value\",\n    yaxis_title=\"Frequency\",\n    showlegend = False,\n    font = dict(\n            size = 14\n            ),    \n    title={\n        'text': \"Yield Distribution in `original`\",\n        'y':0.95,\n        'x':0.5\n        }\n    )\n\n# Display\nfig.show() # for Kaggle version","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:35.72434Z","iopub.execute_input":"2023-05-05T01:45:35.724991Z","iopub.status.idle":"2023-05-05T01:45:35.782487Z","shell.execute_reply.started":"2023-05-05T01:45:35.724961Z","shell.execute_reply":"2023-05-05T01:45:35.781514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create figure\nfig = px.imshow(df_train.corr(), template='simple_white')\n\n# Set Title and x/y axis labels\nfig.update_layout(\n    showlegend = False,\n    font = dict(\n            size = 14\n            ),    \n    title={\n        'text': \"Train Dataset Correlation\",\n        'y':0.98,\n        'x':0.49\n        }\n    )\n\n# Display\nfig.show() ","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:35.784103Z","iopub.execute_input":"2023-05-05T01:45:35.784441Z","iopub.status.idle":"2023-05-05T01:45:35.908866Z","shell.execute_reply.started":"2023-05-05T01:45:35.784411Z","shell.execute_reply":"2023-05-05T01:45:35.908013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create figure\nfig = px.imshow(original.corr(), template='simple_white')\n\n# Set Title and x/y axis labels\nfig.update_layout(\n    showlegend = False,\n    font = dict(\n            size = 14\n            ),    \n    title={\n        'text': \"Original Dataset Correlation\",\n        'y':0.98,\n        'x':0.49\n        }\n    )\n\n# Display\nfig.show() ","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:35.912529Z","iopub.execute_input":"2023-05-05T01:45:35.913093Z","iopub.status.idle":"2023-05-05T01:45:35.958062Z","shell.execute_reply.started":"2023-05-05T01:45:35.913061Z","shell.execute_reply":"2023-05-05T01:45:35.957145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot function\ndef plot_column_distribution(df, column_name):\n    \"\"\"plot a distribution of certain column with [column_name] from [df] dataframe\"\"\"\n\n    # Create figure\n    fig = px.histogram(df[column_name],\n                       template = 'simple_white',\n                       color_discrete_sequence = ['#1192AA'])\n\n    # Set Title and x/y axis labels\n    fig.update_layout(\n        xaxis_title=\"Value\",\n        yaxis_title=\"Frequency\",\n        showlegend = False,\n        font = dict(\n                size = 14\n                ),    \n        title={\n            'text': column_name,\n            'y':0.95,\n            'x':0.5\n            }\n        )\n\n    # Display\n    fig.show()\n\nfor column in df_train.columns[0:-1]:\n    plot_column_distribution(df_train, column)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:35.959525Z","iopub.execute_input":"2023-05-05T01:45:35.959844Z","iopub.status.idle":"2023-05-05T01:45:36.802587Z","shell.execute_reply.started":"2023-05-05T01:45:35.959815Z","shell.execute_reply":"2023-05-05T01:45:36.801671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Feature Engineering</h1>","metadata":{}},{"cell_type":"code","source":"def add_features(df_in):\n    df = df_in.copy(deep = True)\n    \n    df[\"fruit_seed\"] = df[\"fruitset\"] * df[\"seeds\"]\n    return df\n\ndf_train = add_features(df_train)\ndf_test = add_features(df_test)\noriginal = add_features(original)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:36.803798Z","iopub.execute_input":"2023-05-05T01:45:36.8045Z","iopub.status.idle":"2023-05-05T01:45:36.813989Z","shell.execute_reply.started":"2023-05-05T01:45:36.804469Z","shell.execute_reply":"2023-05-05T01:45:36.813105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Preprocess</h1>","metadata":{}},{"cell_type":"code","source":"# Concatenate train and original dataframes, and prepare train and test sets\ndf_train = pd.concat([df_train, original])\nX_train = df_train.drop([f'{target_col}'],axis=1).reset_index(drop=True)\ny_train = df_train[f'{target_col}'].reset_index(drop=True)\nX_test = df_test.reset_index(drop=True)\n\n# StandardScaler\ncategorical_columns = ['is_generated']\nnumeric_columns = [_ for _ in X_train.columns if _ not in categorical_columns]\nsc = MinMaxScaler()\nX_train[numeric_columns] = sc.fit_transform(X_train[numeric_columns])\nX_test[numeric_columns] = sc.transform(X_test[numeric_columns])\n\n# # Randomly sample 80% of the data\n# X_train = X_train.sample(frac=0.8, random_state=42)\n\n# pca = PCA(n_components=3)\n# pca_train = pca.fit_transform(X_train)\n# pca_test = pca.fit_transform(X_test)\n\n# X_train= X_train.join(pd.DataFrame(pca_train))\n# X_test= X_test.join(pd.DataFrame(pca_train))\n\n# X_train.columns = X_train.columns.astype(str)\n# X_test.columns = X_test.columns.astype(str)\n\nprint(f\"X_train shape :{X_train.shape} , y_train shape :{y_train.shape}\")\nprint(f\"X_test shape :{X_test.shape}\")\n\n# Delete the train and test dataframes to free up memory\ndel df_train, df_test, original\n\nX_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:36.815435Z","iopub.execute_input":"2023-05-05T01:45:36.816444Z","iopub.status.idle":"2023-05-05T01:45:36.864639Z","shell.execute_reply.started":"2023-05-05T01:45:36.816377Z","shell.execute_reply":"2023-05-05T01:45:36.863759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Models</h1>","metadata":{}},{"cell_type":"markdown","source":"Thanks to https://www.kaggle.com/tetsutani","metadata":{}},{"cell_type":"code","source":"class Splitter:\n    def __init__(self, kfold=True, n_splits=5):\n        self.n_splits = n_splits\n        self.kfold = kfold\n\n    def split_data(self, X, y, random_state_list):\n        if self.kfold:\n            for random_state in random_state_list:\n                kf = KFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n                for train_index, val_index in kf.split(X, y):\n                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n                    yield X_train, X_val, y_train, y_val\n        else:\n            raise ValueError(f\"Invalid kfold: Must be True\")","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:36.866065Z","iopub.execute_input":"2023-05-05T01:45:36.86698Z","iopub.status.idle":"2023-05-05T01:45:36.874325Z","shell.execute_reply.started":"2023-05-05T01:45:36.866949Z","shell.execute_reply":"2023-05-05T01:45:36.873135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X_train\ny = y_train\n\ndef objective(trial):\n    params = {\n        'n_estimators': 250,\n        'num_leaves': trial.suggest_int('num_leaves', 8, 128),\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n        'subsample': trial.suggest_uniform('subsample', 0.1, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-9, 10.0),\n        'objective': 'regression_l1',\n        'metric': 'mean_absolute_error',\n        'boosting_type': 'gbdt',\n        'device': 'cpu',\n        'random_state': 42\n    }\n    \n    lgbm = lgb.LGBMRegressor(**params)\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    mae_list = []\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        lgbm.fit(X_train, y_train)\n        y_pred = lgbm.predict(X_test)\n        mae = mean_absolute_error(y_test, y_pred)\n        mae_list.append(mae)\n    return np.mean(mae_list)\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:36.875908Z","iopub.execute_input":"2023-05-05T01:45:36.876281Z","iopub.status.idle":"2023-05-05T01:45:38.343657Z","shell.execute_reply.started":"2023-05-05T01:45:36.876237Z","shell.execute_reply":"2023-05-05T01:45:38.342889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X_train\ny = y_train\n\ndef objective(trial):\n    params = {\n        'n_estimators': 250,\n        'depth': trial.suggest_int('depth', 3, 12),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-9, 10.0),\n        'random_strength': trial.suggest_uniform('random_strength', 0.01, 1.0),\n        'max_bin': trial.suggest_int('max_bin', 50, 500),\n        'od_wait': trial.suggest_int('od_wait', 10, 100),\n        'grow_policy': 'Lossguide',\n        'bootstrap_type': 'Bayesian',\n        'od_type': 'Iter',\n        'eval_metric': 'MAE',\n        'loss_function': 'MAE',\n        'random_state': 42\n    }\n    cb = CatBoostRegressor(**params)\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    mae_list = []\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        cb.fit(X_train, y_train, verbose=False)\n        y_pred = cb.predict(X_test)\n        mae = mean_absolute_error(y_test, y_pred)\n        mae_list.append(mae)\n    return np.mean(mae_list)\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:38.347423Z","iopub.execute_input":"2023-05-05T01:45:38.349774Z","iopub.status.idle":"2023-05-05T01:45:50.410529Z","shell.execute_reply.started":"2023-05-05T01:45:38.349745Z","shell.execute_reply":"2023-05-05T01:45:50.40957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X_train\ny = y_train\n\ndef objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n        'max_depth': trial.suggest_int('max_depth', 3, 20),\n        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'bootstrap': True,\n        'random_state': 42\n    }\n    rf = RandomForestRegressor(**params)\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    mae_list = []\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        rf.fit(X_train, y_train)\n        y_pred = rf.predict(X_test)\n        mae = mean_absolute_error(y_test, y_pred)\n        mae_list.append(mae)\n    return np.mean(mae_list)\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:45:50.412096Z","iopub.execute_input":"2023-05-05T01:45:50.412784Z","iopub.status.idle":"2023-05-05T01:45:58.733217Z","shell.execute_reply.started":"2023-05-05T01:45:50.412751Z","shell.execute_reply":"2023-05-05T01:45:58.732305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Regressor:\n    def __init__(self, n_estimators=100, device=\"cpu\", random_state=0):\n        self.n_estimators = n_estimators\n        self.device = device\n        self.random_state = random_state\n        self.models = self._define_model()\n        self.models_name = list(self._define_model().keys())\n        self.len_models = len(self.models)\n        \n    def _define_model(self):\n        \n        xgb_params = {\n            'n_estimators': self.n_estimators,\n            'max_depth': 7,\n            'learning_rate': 0.0116,\n            'colsample_bytree': 1,\n            'subsample': 0.6085,\n            'min_child_weight': 9,\n            'reg_lambda': 4.879e-07,\n            'max_bin': 431,\n            'n_jobs': -1,\n            'eval_metric': 'mae',\n            'objective': \"reg:squarederror\",\n            'verbosity': 0,\n            'random_state': self.random_state,\n        }\n        if self.device == 'gpu':\n            xgb_params['tree_method'] = 'gpu_hist'\n            xgb_params['predictor'] = 'gpu_predictor'\n        xgb_exact_params = xgb_params.copy()\n        xgb_exact_params['tree_method'] = 'exact'\n        xgb_approx_params = xgb_params.copy()\n        xgb_approx_params['tree_method'] = 'approx'\n        \n        lgb_params = {\n            'n_estimators': self.n_estimators,\n            'max_depth': 7,\n            \"num_leaves\": 16,\n            'learning_rate': 0.05,\n            'subsample': 0.60,\n            'colsample_bytree': 1,\n            'reg_alpha': 0.25,\n            'reg_lambda': 5e-07,\n            'objective': 'regression_l1',\n            'metric': 'mean_absolute_error',\n            'boosting_type': 'gbdt',\n            'device': self.device,\n            'random_state': self.random_state\n        }\n        lgb2_params = {\n            'n_estimators': self.n_estimators,\n            'num_leaves': 93, \n            'min_child_samples': 20, \n            'learning_rate': 0.05533790147941807, \n            'colsample_bytree': 0.8809128870084636, \n            'reg_alpha': 0.0009765625, \n            'reg_lambda': 0.015589408048174165,\n            'objective': 'regression_l1',\n            'metric': 'mean_absolute_error',\n            'boosting_type': 'gbdt',\n            'device': self.device,\n            'random_state': self.random_state\n        }\n        lgb3_params = {\n            'n_estimators': self.n_estimators,\n            'num_leaves': 45,\n            'max_depth': 13,\n            'learning_rate': 0.0684383311038932,\n            'subsample': 0.5758412171285148,\n            'colsample_bytree': 0.8599714680300794,\n            'reg_lambda': 1.597717830931487e-08,\n            'objective': 'regression_l1',\n            'metric': 'mean_absolute_error',\n            'boosting_type': 'gbdt',\n            'device': self.device,\n            'random_state': self.random_state,\n            'force_col_wise': True\n        }\n        lgb_goss_params = lgb_params.copy()\n        lgb_goss_params['boosting_type'] = 'goss'\n        lgb_dart_params = lgb_params.copy()\n        lgb_dart_params['boosting_type'] = 'dart'\n        lgb_dart_params['n_estimators'] = 500\n                \n        cb_params = {\n            'iterations': self.n_estimators,\n            'depth': 8,\n            'learning_rate': 0.01,\n            'l2_leaf_reg': 0.7,\n            'random_strength': 0.2,\n            'max_bin': 200,\n            'od_wait': 65,\n            'one_hot_max_size': 70,\n            'grow_policy': 'Depthwise',\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'Iter',\n            'eval_metric': 'MAE',\n            'loss_function': 'MAE',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state\n        }\n        cb2_params = {\n            'iterations': self.n_estimators,\n            'depth': 9, \n            'learning_rate': 0.456,\n            'l2_leaf_reg': 8.41,\n            'random_strength': 0.18,\n            'max_bin': 225, \n            'od_wait': 58, \n            'grow_policy': 'Lossguide',\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'Iter',\n            'eval_metric': 'MAE',\n            'loss_function': 'MAE',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state\n        }\n        cb3_params = {\n            'n_estimators': self.n_estimators,\n            'depth': 11,\n            'learning_rate': 0.08827842054729117,\n            'l2_leaf_reg': 4.8351074756668864e-05,\n            'random_strength': 0.21306687539993183,\n            'max_bin': 483,\n            'od_wait': 97,\n            'grow_policy': 'Lossguide',\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'Iter',\n            'eval_metric': 'MAE',\n            'loss_function': 'MAE',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state,\n            'silent': True\n        }\n        cb_sym_params = cb_params.copy()\n        cb_sym_params['grow_policy'] = 'SymmetricTree'\n        cb_loss_params = cb_params.copy()\n        cb_loss_params['grow_policy'] = 'Lossguide'\n        \n        models = {\n            #\"xgb\": xgb.XGBRegressor(**xgb_params),\n            #\"xgb_exact\": xgb.XGBRegressor(**xgb_exact_params),\n            #\"xgb_approx\": xgb.XGBRegressor(**xgb_approx_params),\n            \"lgb\": lgb.LGBMRegressor(**lgb_params),\n            \"lgb2\": lgb.LGBMRegressor(**lgb2_params),\n            \"lgb3\": lgb.LGBMRegressor(**lgb3_params),\n            \"cat\": CatBoostRegressor(**cb_params),\n            \"cat2\": CatBoostRegressor(**cb2_params),\n            \"cat3\": CatBoostRegressor(**cb3_params),\n            #\"cat_sym\": CatBoostRegressor(**cb_sym_params),\n            \"cat_loss\": CatBoostRegressor(**cb_loss_params),\n            #\"Ridge\": RidgeCV(),\n            #\"Lasso\": LassoCV(),\n            \"RandomForestRegressor\": RandomForestRegressor(n_estimators=200, random_state=self.random_state, n_jobs=-1),\n            #\"PLSRegression\": PLSRegression(n_components=10, max_iter=2000),\n            \"PassiveAggressiveRegressor\": PassiveAggressiveRegressor(max_iter=3000, tol=1e-3, n_iter_no_change=30, random_state=self.random_state),\n            #\"GradientBoostingRegressor\": GradientBoostingRegressor(n_estimators=2000, learning_rate=0.05, loss=\"absolute_error\", random_state=self.random_state),\n            \"HistGradientBoostingRegressor\": HistGradientBoostingRegressor(max_iter=self.n_estimators, learning_rate=0.01, loss=\"absolute_error\", n_iter_no_change=300,random_state=self.random_state),\n            #\"ARDRegression\": ARDRegression(n_iter=1000),\n            \"HuberRegressor\": HuberRegressor(max_iter=3000),\n            \"KNeighborsRegressor\": KNeighborsRegressor(n_neighbors=5, n_jobs=-1)\n        }\n        \n        return models","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:54:05.822097Z","iopub.execute_input":"2023-05-05T01:54:05.822842Z","iopub.status.idle":"2023-05-05T01:54:05.839931Z","shell.execute_reply.started":"2023-05-05T01:54:05.822798Z","shell.execute_reply":"2023-05-05T01:54:05.838896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OptunaWeights:\n    def __init__(self, random_state: int = 42, n_trials: int = 2000):\n        self.study = None\n        self.weights = None\n        self.random_state = random_state\n        self.n_trials = n_trials\n\n    def _objective(self, trial: optuna.trial.Trial, y_true: np.ndarray, y_preds: List[np.ndarray]) -> float:\n        # Define the weights for the predictions from each model\n        weights = np.array([trial.suggest_float(f\"weight{n}\", 1e-14, 1) for n in range(len(y_preds))])\n\n        # Calculate the weighted prediction\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n\n        # Calculate the score for the weighted prediction\n        score = mean_absolute_error(y_true, weighted_pred)\n        return score\n\n    def fit(self, y_true: np.ndarray, y_preds: List[np.ndarray]) -> None:\n        optuna.logging.set_verbosity(optuna.logging.ERROR)\n        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n        pruner = optuna.pruners.HyperbandPruner()\n        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='minimize')\n        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n        self.study.optimize(objective_partial, n_trials=self.n_trials)\n        self.weights = np.array([self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))])\n\n    def predict(self, y_preds: List[np.ndarray]) -> np.ndarray:\n        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n        return weighted_pred\n\n    def fit_predict(self, y_true: np.ndarray, y_preds: List[np.ndarray]) -> np.ndarray:\n        self.fit(y_true, y_preds)\n        return self.predict(y_preds)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:54:06.482319Z","iopub.execute_input":"2023-05-05T01:54:06.482944Z","iopub.status.idle":"2023-05-05T01:54:06.492196Z","shell.execute_reply.started":"2023-05-05T01:54:06.482916Z","shell.execute_reply":"2023-05-05T01:54:06.491285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = True\nn_splits = 1 if not kfold else 5\nrandom_state = 42\nrandom_state_list = [42]\nn_estimators = 9999\nearly_stopping_rounds = 333\nverbose = False\ndevice = 'cpu'\nunique_targets = np.unique(y_train)\n\nsplitter = Splitter(kfold=kfold, n_splits=n_splits)\n\n# Initialize an array for storing test predictions\nregressor = Regressor(n_estimators, device, random_state)\ntest_predss = np.zeros((X_test.shape[0]))\noof_predss = np.zeros((X_train.shape[0]))\nensemble_score = []\nweights = []\ntrained_models = {'lgb_test':[], 'cat_test':[], \"rf_test\":[]}\nscore_dict = dict(zip(regressor.models_name, [[] for _ in range(regressor.len_models)]))\n\n    \nfor i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n    n = i % n_splits\n    m = i // n_splits\n            \n    # Get a set of Regressor models\n    regressor = Regressor(n_estimators, device, random_state)\n    models = regressor.models\n    \n    # Initialize lists to store oof and test predictions for each base model\n    oof_preds = []\n    test_preds = []\n    \n    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n    for name, model in models.items():\n        if ('xgb' in name) or ('lgb' in name) or ('cat' in name):\n            model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n        else:\n            model.fit(X_train_, y_train_)\n            \n        if name in trained_models.keys():\n            trained_models[f'{name}'].append(deepcopy(model))\n        \n        test_pred = model.predict(X_test).reshape(-1)\n        y_val_pred = model.predict(X_val).reshape(-1)\n        \n        y_val_pred = [min(unique_targets, key = lambda x: abs(x - pred)) for pred in y_val_pred]\n        test_pred = [min(unique_targets, key = lambda x: abs(x - pred)) for pred in test_pred]\n        \n        score = mean_absolute_error(y_val, y_val_pred)\n        score_dict[name].append(score)\n        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] MAE score: {score:.5f}')\n        \n        oof_preds.append(y_val_pred)\n        test_preds.append(test_pred)\n    \n    # Use Optuna to find the best ensemble weights\n    optweights = OptunaWeights(random_state=random_state)\n    y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n    \n    score = mean_absolute_error(y_val, y_val_pred)\n    print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] MAE score {score:.5f}')\n    ensemble_score.append(score)\n    weights.append(optweights.weights)\n    \n    # Predict to X_test by the best ensemble weights\n    test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n    oof_predss[X_val.index] = optweights.predict(oof_preds)\n    \n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:54:27.787718Z","iopub.execute_input":"2023-05-05T01:54:27.788071Z","iopub.status.idle":"2023-05-05T01:55:38.97949Z","shell.execute_reply.started":"2023-05-05T01:54:27.788045Z","shell.execute_reply":"2023-05-05T01:55:38.978203Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the mean LogLoss score of the ensemble\nmean_score = np.mean(ensemble_score)\nstd_score = np.std(ensemble_score)\nprint(f'Ensemble MAE score {mean_score:.5f} ± {std_score:.5f}')\n\nprint('')\n# Print the mean and standard deviation of the ensemble weights for each model\nprint('--- Model Weights ---')\nmean_weights = np.mean(weights, axis=0)\nstd_weights = np.std(weights, axis=0)\nfor name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n    print(f'{name}: {mean_weight:.5f} ± {std_weight:.5f}')","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:47:18.480728Z","iopub.status.idle":"2023-05-05T01:47:18.481805Z","shell.execute_reply.started":"2023-05-05T01:47:18.481574Z","shell.execute_reply":"2023-05-05T01:47:18.481597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style = \"font-family: Georgia;font-weight: bold; font-size: 30px; color: #1192AA; text-align:left\">Make Submission</h1>","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(PATH_SUB)\nsub[f'{target_col}'] = test_predss\n\nsub.to_csv('submission.csv', index=False)\nsub","metadata":{"execution":{"iopub.status.busy":"2023-05-05T01:47:18.483005Z","iopub.status.idle":"2023-05-05T01:47:18.485595Z","shell.execute_reply.started":"2023-05-05T01:47:18.485365Z","shell.execute_reply":"2023-05-05T01:47:18.485387Z"},"trusted":true},"execution_count":null,"outputs":[]}]}